#!/usr/bin/env python3

import subprocess
import json
import os
import re
import urllib.request
from datetime import datetime, timezone
from typing import List, Dict

def run_gh_command(args: List[str]) -> str:
    result = subprocess.run(['gh'] + args, capture_output=True, text=True, check=True)
    return result.stdout

def get_check_update_time() -> tuple[datetime, str]:
    print("ğŸ” Getting last check update action time...")
    try:
        output = run_gh_command(['run', 'list', '--workflow=check-update.yml', '--limit=1', '--json=databaseId,createdAt'])
        runs = json.loads(output)
        if not runs:
            raise Exception("No check-update workflow runs found")
        run = runs[0]
        run_id = run['databaseId']
        created_at = run['createdAt']
        check_time = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
        print(f"   Last check update time: {check_time.isoformat()} (run_id: {run_id})")
        return check_time, run_id
    except subprocess.CalledProcessError as e:
        print(f"  gh command failed: {e.stderr}")
        raise
    except Exception as e:
        print(f"  Error: {e}")
        raise

def get_build_test_runs_since(check_time: datetime) -> List[Dict]:
    print(f"ğŸ” Finding build test runs since {check_time.isoformat()}...")
    try:
        output = run_gh_command(['run', 'list', '--workflow=build.yml', '--limit=50', '--json=databaseId,displayTitle,createdAt,status,conclusion'])
        runs = json.loads(output)
        recent_runs = []
        for run in runs:
            created_at = datetime.fromisoformat(run['createdAt'].replace('Z', '+00:00'))
            if created_at > check_time:
                run['createdAt_dt'] = created_at
                recent_runs.append(run)
        print(f"   Found {len(recent_runs)} build test runs after specified time")
        return recent_runs
    except subprocess.CalledProcessError as e:
        print(f"  gh command failed: {e.stderr}")
        return []
    except Exception as e:
        print(f"  Error: {e}")
        return []

def extract_package_name(title: str) -> str:
    if title.startswith('Build test for '):
        parts = title[15:].split(' ', 1)
        return parts[0] if parts else ""
    return ""

def query_aur_packages(package_names: List[str]) -> Dict[str, tuple]:
    if not package_names:
        return {}
    print(f"ğŸŒ Querying AUR for {len(package_names)} packages last update time and maintainer info...")
    query_parts = ["v=5", "type=info"]
    for pkg in package_names:
        query_parts.append(f"arg[]={urllib.request.quote(pkg)}")
    url = f"https://aur.archlinux.org/rpc/?{'&'.join(query_parts)}"
    with urllib.request.urlopen(url, timeout=30) as response:
        data = json.loads(response.read().decode())
        aur_info = {}
        bot_identifiers = ['AutoUpdateBot', 'auto-update-bot@arch4edu.org', 'arch4edu']
        for pkg_info in data.get('results', []):
            name = pkg_info.get('Name')
            last_modified = pkg_info.get('LastModified')
            maintainer = pkg_info.get('Maintainer') or ''
            comaintainers = pkg_info.get('CoMaintainers') or []
            if name and last_modified:
                is_co_maintainer = False
                all_maintainers = [maintainer] + comaintainers
                for maint in all_maintainers:
                    if not maint:
                        continue
                    for bot_id in bot_identifiers:
                        if bot_id in maint:
                            is_co_maintainer = True
                            break
                    if is_co_maintainer:
                        break
                aur_info[name] = (
                    datetime.fromtimestamp(last_modified, tz=timezone.utc),
                    is_co_maintainer
                )
        print(f"   Successfully retrieved AUR info for {len(aur_info)}/{len(package_names)} packages")
        return aur_info

def get_check_run_info(run_id: str) -> dict:
    """è§£æ check-update run æ—¥å¿—ï¼Œæå– aur_missing å’Œ nvchecker_failed çš„åŒ…é›†åˆ"""
    try:
        log_output = run_gh_command(['run', 'view', str(run_id), '--log'])
        lines = log_output.split('\n')
        
        aur_missing_packages = set()
        nvchecker_failed_packages = set()
        in_process_updates = False
        
        for line in lines:
            # æ£€æµ‹ Process updates æ­¥éª¤
            if 'Process updates' in line and 'python process-update.py' in line:
                in_process_updates = True
                continue
            elif in_process_updates and line.startswith('update	'):
                # è§£æ "doesn't exist on AUR"
                if "doesn't exist on AUR" in line:
                    # æ ¼å¼: "update	Process updates	HH:MM:SS.mmsZ python-librosa doesn't exist on AUR."
                    parts = line.split()
                    # æŸ¥æ‰¾åŒ…åï¼ˆé€šå¸¸åœ¨ "doesn't exist" ä¹‹å‰ï¼‰
                    for i, part in enumerate(parts):
                        if "doesn't" in part or "exist" in part:
                            if i > 0:
                                pkg = parts[i-1]
                                aur_missing_packages.add(pkg)
                            break
                # è§£æ "Failed to check update for <pkg>: event=..."
                elif "Failed to check update for" in line:
                    # æ ¼å¼: "update	Process updates	HH:MM:SS.mmsZ Failed to check update for twitch-dl: event=running cmd."
                    # æå–åŒ…å
                    import re
                    match = re.search(r'Failed to check update for (\S+):', line)
                    if match:
                        pkg = match.group(1)
                        # æ’é™¤ event=running cmd çš„æƒ…å†µï¼ˆå·²è¢« process-update å¿½ç•¥ï¼‰
                        if "event=running cmd" not in line:
                            nvchecker_failed_packages.add(pkg)
                # æ£€æµ‹æ˜¯å¦ç¦»å¼€ Process updates æ­¥éª¤
                elif line.startswith('update	Post Run'):
                    in_process_updates = False
        
        return {
            'aur_missing': aur_missing_packages,
            'nvchecker_failed': nvchecker_failed_packages
        }
    except Exception as e:
        print(f"   Error getting check run info for {run_id}: {e}")
        return {'aur_missing': set(), 'nvchecker_failed': set()}

def get_run_info(run_id: str) -> dict:
    """ä¸€æ¬¡ log è°ƒç”¨ï¼ŒåŒæ—¶è§£æ build errorã€push conclusion"""
    try:
        log_output = run_gh_command(['run', 'view', str(run_id), '--log'])
        lines = log_output.split('\n')

        build_error = "No==>ERRORerrors"
        push_conclusion = ''
        in_push_job = False
        push_job_seen = False
        dep_error_lines = []  # æ”¶é›†åŒ…å«ä¾èµ–é”™è¯¯çš„è¡Œ
        dep_specific_lines = []  # æ›´å…·ä½“çš„ä¾èµ–é”™è¯¯ï¼ˆ pacman çš„ï¼‰

        for line in lines:
            # Build error detection
            if build_error == "No==>ERRORerrors":
                if '==> ERROR:' in line:
                    error_text = line.split('==> ERROR:')[1].strip()
                    if error_text:
                        build_error = error_text
                elif 'is greater than newver' in line:
                    error_text = line.strip()
                    if error_text:
                        build_error = error_text

            # Collect dependency-related errors (only from ==> ERROR: lines)
            if '==> ERROR:' in line:
                error_text = line.split('==> ERROR:')[1].strip()
                if any(keyword in error_text.lower() for keyword in [
                    'failed to install missing dependencies',
                    'could not resolve all dependencies',
                ]):
                    dep_specific_lines.append(error_text)
                    dep_error_lines.append(error_text)  # also add to general

            # Push job detection & conclusion
            if line.startswith('push\t'):
                in_push_job = True
                push_job_seen = True
            elif line.startswith('build\t'):
                in_push_job = False
            elif line.startswith('##[error]') and in_push_job:
                push_conclusion = 'failure'

        # ä¼˜å…ˆä½¿ç”¨æ›´å…·ä½“çš„ pacman ä¾èµ–é”™è¯¯
        if dep_specific_lines:
            build_error = dep_specific_lines[0]
        elif dep_error_lines:
            build_error = dep_error_lines[0]

        # å¦‚æœ push job å­˜åœ¨ä¸”æœªå‘ç° errorï¼Œè§†ä¸º success
        if push_job_seen and not push_conclusion:
            push_conclusion = 'success'

        return {'build_error': build_error, 'push_conclusion': push_conclusion}
    except Exception as e:
        print(f"   Error getting run info for {run_id}: {e}")
        return {'build_error': f"Failed: {e}", 'push_conclusion': ''}

def get_manual_fix_commits_since(check_time: datetime) -> set:
    """æ£€æŸ¥ check_time ä¹‹åçš„æäº¤ï¼Œæ‰¾å‡ºä¿®æ”¹äº† config/ ç›®å½•ä¸‹æ–‡ä»¶çš„æäº¤ï¼Œä»ä¸­æå–åŒ…å"""
    print("ğŸ” Checking for fixed packages by post-check commits...")
    try:
        # ä½¿ç”¨ git log æŸ¥æ‰¾ check_time ä¹‹åçš„æäº¤ï¼Œæ ¼å¼ï¼š<hash>|<author>|<date>|<subject>
        # æ³¨æ„ï¼šgit log --since ä½¿ç”¨ ISO 8601 UTC æ—¶é—´ï¼ˆä»¥ Z ç»“å°¾ï¼‰ï¼Œç¡®ä¿è·¨æ—¶åŒºä¸€è‡´æ€§
        since_time_utc = check_time.strftime('%Y-%m-%dT%H:%M:%SZ')
        result = subprocess.run(
            ['git', 'log', f'--since={since_time_utc}', '--format=%H|%an|%ai|%s', '--name-only'],
            cwd='/home/petron/auto_update_bot/aur-auto-update',
            capture_output=True, text=True, check=False
        )
        lines = result.stdout.split('\n')
        fixed_packages = set()
        current_commit_files = []
        in_files_section = False

        for line in lines:
            line_stripped = line.strip()
            if not line_stripped:
                continue
            # ä½¿ç”¨ | åˆ†éš”ç¬¦åˆ¤æ–­æ˜¯å¦ä¸º commit å¤´
            if '|' in line:
                # å¤„ç†ä¸Šä¸€ä¸ªæäº¤çš„æ–‡ä»¶åˆ—è¡¨
                if current_commit_files:
                    fixed_packages.update(extract_packages_from_paths(current_commit_files))
                    current_commit_files = []
                # è§£ææ–° commit å¤´ï¼šæ ¼å¼ <hash>|<author>|<date>|<subject>
                parts = line.split('|', 3)
                if len(parts) >= 4:
                    commit_hash, author, date, subject = parts
                    if is_github_action_author(author):
                        in_files_section = False  # è·³è¿‡æ­¤æäº¤çš„æ–‡ä»¶
                        continue
                in_files_section = True
            elif in_files_section and line_stripped:
                # è¿™æ˜¯æ–‡ä»¶è·¯å¾„ï¼ˆä¸åŒ…å« |ï¼‰
                current_commit_files.append(line_stripped)

        # å¤„ç†æœ€åä¸€ä¸ªæäº¤
        if current_commit_files:
            fixed_packages.update(extract_packages_from_paths(current_commit_files))

        print(f"   Found {len(fixed_packages)} fixed packages: {sorted(fixed_packages)}")
        return fixed_packages
    except Exception as e:
        print(f"   Error checking commits: {e}")
        return set()

def is_github_action_author(author: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸º GitHub Actions æäº¤"""
    return author == "GitHub Actions" or 'github-actions[bot]' in author

def extract_packages_from_paths(paths: List[str]) -> set:
    """ä»æ–‡ä»¶è·¯å¾„åˆ—è¡¨ä¸­æå–åŒ…åï¼ˆconfig/<maintainer>/<pkg>.yamlï¼‰"""
    packages = set()
    for path in paths:
        # åªå¤„ç† config/ ç›®å½•ä¸‹çš„ yaml æ–‡ä»¶
        if not (path.startswith('config/') and path.endswith('.yaml')):
            continue
        parts = path.split('/')
        if len(parts) >= 3:
            pkg_file = parts[-1]  # <pkg>.yaml
            pkg_name = pkg_file[:-5] if pkg_file.endswith('.yaml') else pkg_file
            packages.add(pkg_name)
    return packages

def process_builds(build_runs: List[Dict], aur_info: Dict[str, tuple], check_time: datetime, check_run_id: str):
    # Get manual fix commits since check time
    fixed_packages = get_manual_fix_commits_since(check_time)
    
    # ä» check-update run ä¸­è·å–æ¯ä¸ªåŒ…çš„é¢å¤–çŠ¶æ€ï¼ˆaur_missing, nvchecker_failedï¼‰
    print("ğŸ” Analyzing check-update run for aur_missing and nvchecker_failed states...")
    check_run_info = get_check_run_info(check_run_id)
    aur_missing_packages = check_run_info.get('aur_missing', set())
    nvchecker_failed_packages = check_run_info.get('nvchecker_failed', set())
    print(f"   Found {len(aur_missing_packages)} packages missing on AUR")
    print(f"   Found {len(nvchecker_failed_packages)} packages with nvchecker failures")

    # Calculate dynamic column widths (no AURUpdate column)
    all_packages = [build['package'] for build in build_runs]
    max_pkg_len = max(len(pkg) for pkg in all_packages) if all_packages else 0
    pkg_width = min(max_pkg_len + 2, 40)  # +2 padding, max 40
    run_id_width = 12
    status_width = 20
    total_width = pkg_width + run_id_width + status_width + 2  # 2 spaces between columns

    print("\n" + "="*total_width)
    print("ğŸ“Š AUR Auto-Update Build Results")
    print("="*total_width)
    header = f"{'Package':<{pkg_width}} {'Run ID':<{run_id_width}} {'Status':<{status_width}}"
    print(header)
    print("-"*total_width)

    total = len(build_runs)
    # Status counts in FINAL ORDER: ğŸ“¦ âœ… ğŸŸ¢ âš« ğŸ”´ ğŸŸ¡ âŒ ğŸš« â¬œ âš ï¸
    fully_successful_count = 0  # ğŸ“¦
    fixed_count = 0             # âœ…
    aur_updated_count = 0       # ğŸŸ¢
    not_maintained_count = 0    # âš«
    dependency_issue_count = 0  # ğŸ”´
    vercmp_failed_count = 0     # ğŸŸ¡
    build_failed_count = 0      # âŒ
    push_failed_count = 0       # ğŸš«
    aur_missing_count = 0       # â¬œ
    nvchecker_failed_count = 0  # âš ï¸

    for build in build_runs:
        pkg = build['package']
        run_id = build['run_id']
        aur_data = aur_info.get(pkg)
        if aur_data:
            aur_time, is_co_maintainer = aur_data
            aur_success = aur_time > check_time if aur_time else False
        else:
            aur_time = None
            is_co_maintainer = False
            aur_success = False

        # è·å– run ä¿¡æ¯ï¼ˆbuild error å’Œ push conclusionï¼‰ï¼Œè‡ªåŠ¨ç¼“å­˜
        run_info = get_run_info(run_id)
        build_error = run_info['build_error']
        push_conclusion = run_info['push_conclusion']
        build_failed = build_error != "No==>ERRORerrors"
        vercmp_failed = "is greater than newver" in build_error.lower()

        # Priority order: ğŸ“¦ âœ… ğŸŸ¢ âš« ğŸŸ¡ ğŸ”´ âŒ ğŸš« â¬œ âš ï¸

        # 1. Fixed
        if pkg in fixed_packages:
            status = "âœ… Fixed"
            fixed_count += 1
        # 2. Non-co-maintainer
        elif not is_co_maintainer:
            status = "âš« No longer maintained"
            not_maintained_count += 1
        # 3. AUR missing (check-update ç¯èŠ‚å‘ç°åŒ…ä¸åœ¨ AUR)
        elif pkg in aur_missing_packages:
            status = "â¬œ AUR missing"
            aur_missing_count += 1
        # 4. nvchecker failed (check-update ç¯èŠ‚æ£€æŸ¥å¤±è´¥)
        elif pkg in nvchecker_failed_packages:
            status = "âš ï¸ nvchecker failed"
            nvchecker_failed_count += 1
        # 5. Co-maintainer: evaluate build results
        else:
            # 5a. vercmp failed
            if vercmp_failed:
                status = "ğŸŸ¡ vercmp failed"
                vercmp_failed_count += 1
            # 5b. Dependency issue
            elif build_failed and any(keyword in build_error.lower() for keyword in [
                'failed to install missing dependencies',
                'could not resolve all dependencies',
            ]):
                status = "ğŸ”´ Dependency issue"
                dependency_issue_count += 1
            # 5c. Build failed but AUR updated -> ğŸŸ¢
            elif build_failed and aur_success:
                status = "ğŸŸ¢ AUR updated"
                aur_updated_count += 1
            # 5d. Build failed -> âŒ
            elif build_failed:
                status = "âŒ Build failed"
                build_failed_count += 1
            # 5e. Push failed -> ğŸš«
            elif push_conclusion and push_conclusion != 'success':
                status = "ğŸš« Push failed"
                push_failed_count += 1
            # 5f. Build succeeded, push succeeded -> Success
            else:
                status = "ğŸ“¦ Success"
                fully_successful_count += 1

        display_name = pkg if len(pkg) <= pkg_width - 3 else pkg[:pkg_width - 6] + "..."
        print(f"{display_name:<{pkg_width}} {run_id:<{run_id_width}} {status:<{status_width}}")

    print("="*total_width)
    # Build summary string with only non-zero counts in priority order
    status_parts = []
    if fully_successful_count > 0:
        status_parts.append(f"ğŸ“¦{fully_successful_count}")
    if fixed_count > 0:
        status_parts.append(f"âœ…{fixed_count}")
    if aur_updated_count > 0:
        status_parts.append(f"ğŸŸ¢{aur_updated_count}")
    if not_maintained_count > 0:
        status_parts.append(f"âš«{not_maintained_count}")
    if vercmp_failed_count > 0:
        status_parts.append(f"ğŸŸ¡{vercmp_failed_count}")
    if dependency_issue_count > 0:
        status_parts.append(f"ğŸ”´{dependency_issue_count}")
    if build_failed_count > 0:
        status_parts.append(f"âŒ{build_failed_count}")
    if push_failed_count > 0:
        status_parts.append(f"ğŸš«{push_failed_count}")
    summary = " ".join(status_parts)
    print(f"Total: {total} packages ({summary})")

def main():
    try:
        print("=" * 60)
        print("ğŸš€ AUR Auto-Update Actions Analysis Script")
        print("=" * 60)
        check_time, check_run_id = get_check_update_time()
        recent_build_runs = get_build_test_runs_since(check_time)
        if not recent_build_runs:
            print("âš ï¸  No build test runs found after check update")
            return
        print("\nğŸ”§ Analyzing build test runs...")
        build_data = []
        package_names = []
        for run in recent_build_runs:
            run_id = run['databaseId']
            title = run.get('displayTitle', '')
            conclusion = run.get('conclusion')
            pkg = extract_package_name(title)
            if pkg and conclusion:
                build_data.append({
                    'run_id': run_id,
                    'package': pkg,
                    'conclusion': conclusion,
                })
                if pkg not in package_names:
                    package_names.append(pkg)
        if not build_data:
            print("âš ï¸  Could not extract valid package names from build test runs")
            return
        print(f"   Extracted {len(build_data)} build records for {len(package_names)} unique packages")
        
        # Sort by package name for consistent output
        build_data.sort(key=lambda x: x['package'])
        
        aur_info = query_aur_packages(package_names)
        process_builds(build_data, aur_info, check_time, check_run_id)
    except Exception as e:
        print(f"\nâŒ Script execution failed: {e}")
        import traceback
        traceback.print_exc()
        return

if __name__ == '__main__':
    main()
